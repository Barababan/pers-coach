#!/usr/bin/env python3
"""
Extract 3D Pose from Trainer Video using SAM 3D Body

–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:
- –í–∏–¥–µ–æ —Ç—Ä–µ–Ω–µ—Ä–∞ (squat_cropped.mp4)

–í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:
- JSON —Å 3D landmarks –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–∞–¥—Ä–∞ (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å MediaPipe)
- NPZ —Å –ø–æ–ª–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –ø–æ–∑—ã –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ 3D —á–µ–ª–æ–≤–µ—á–∫–∞

–î–ª—è –∑–∞–ø—É—Å–∫–∞ –Ω–∞ Lightning.AI —Å GPU
"""

import os
import sys
import json
import argparse
import numpy as np
import cv2
from tqdm import tqdm
import torch

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ SAM 3D Body
SAM3D_PATH = os.path.join(os.path.dirname(__file__), "sam-3d-body")
sys.path.insert(0, SAM3D_PATH)

from sam_3d_body import load_sam_3d_body, SAM3DBodyEstimator


# –ú–∞–ø–ø–∏–Ω–≥ SAM 3D Body joints –∫ MediaPipe Pose landmarks (33 —Ç–æ—á–∫–∏)
# SAM 3D Body –∏—Å–ø–æ–ª—å–∑—É–µ—Ç MHR (Momentum Human Rig) —Ñ–æ—Ä–º–∞—Ç
# MediaPipe –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 33 landmarks

MEDIAPIPE_LANDMARKS = [
    "nose", "left_eye_inner", "left_eye", "left_eye_outer",
    "right_eye_inner", "right_eye", "right_eye_outer",
    "left_ear", "right_ear", "mouth_left", "mouth_right",
    "left_shoulder", "right_shoulder", "left_elbow", "right_elbow",
    "left_wrist", "right_wrist", "left_pinky", "right_pinky",
    "left_index", "right_index", "left_thumb", "right_thumb",
    "left_hip", "right_hip", "left_knee", "right_knee",
    "left_ankle", "right_ankle", "left_heel", "right_heel",
    "left_foot_index", "right_foot_index"
]

# –û—Å–Ω–æ–≤–Ω—ã–µ —Å—É—Å—Ç–∞–≤—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø–æ–∑ (—Å–æ–≤–ø–∞–¥–∞—é—Ç —Å MediaPipe)
POSE_JOINTS = {
    'nose': 0,
    'left_shoulder': 11,
    'right_shoulder': 12,
    'left_elbow': 13,
    'right_elbow': 14,
    'left_wrist': 15,
    'right_wrist': 16,
    'left_hip': 23,
    'right_hip': 24,
    'left_knee': 25,
    'right_knee': 26,
    'left_ankle': 27,
    'right_ankle': 28,
}


def setup_estimator(checkpoint_path, mhr_path, device='cuda'):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SAM 3D Body estimator"""
    print(f"üîß Loading SAM 3D Body on {device}...")
    
    model, model_cfg = load_sam_3d_body(
        checkpoint_path, 
        device=torch.device(device),
        mhr_path=mhr_path
    )
    
    estimator = SAM3DBodyEstimator(
        sam_3d_body_model=model,
        model_cfg=model_cfg,
        human_detector=None,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω—ã–π –∫–∞–¥—Ä (—Ç—Ä–µ–Ω–µ—Ä —É–∂–µ –≤ —Ü–µ–Ω—Ç—Ä–µ)
        human_segmentor=None,
        fov_estimator=None,
    )
    
    print("‚úÖ SAM 3D Body loaded!")
    return estimator


def sam3d_to_mediapipe_format(sam3d_output):
    """
    –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≤—ã—Ö–æ–¥ SAM 3D Body –≤ —Ñ–æ—Ä–º–∞—Ç MediaPipe World Landmarks
    
    SAM 3D Body output —Å–æ–¥–µ—Ä–∂–∏—Ç:
    - pred_keypoints_3d: 3D –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤ –º–µ—Ç—Ä–∞—Ö
    - pred_keypoints_2d: 2D –ø—Ä–æ–µ–∫—Ü–∏–∏ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    - pred_joint_coords: –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Å—É—Å—Ç–∞–≤–æ–≤ MHR rig
    """
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º pred_joint_coords - —ç—Ç–æ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Å—É—Å—Ç–∞–≤–æ–≤ —Å–∫–µ–ª–µ—Ç–∞
    joints_3d = sam3d_output.get('pred_joint_coords')
    
    if joints_3d is None:
        # Fallback –Ω–∞ pred_keypoints_3d
        joints_3d = sam3d_output.get('pred_keypoints_3d')
    
    if joints_3d is None:
        return None
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã (—Ü–µ–Ω—Ç—Ä–∏—Ä—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ hip)
    # MediaPipe world landmarks —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω—ã –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ hip center
    
    # SAM 3D Body –∏–º–µ–µ—Ç –¥—Ä—É–≥—É—é —Ç–æ–ø–æ–ª–æ–≥–∏—é —Å–∫–µ–ª–µ—Ç–∞, 
    # –Ω—É–∂–Ω–æ –º–∞–ø–ø–∏—Ç—å –Ω–∞ MediaPipe 33 landmarks
    
    # –î–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Å—É—Å—Ç–∞–≤—ã —Ç–µ–ª–∞
    # –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø–æ–∑
    
    result = {
        'joints_3d': joints_3d.tolist() if isinstance(joints_3d, np.ndarray) else joints_3d,
        'keypoints_3d': sam3d_output.get('pred_keypoints_3d', np.zeros((1, 3))).tolist(),
        'keypoints_2d': sam3d_output.get('pred_keypoints_2d', np.zeros((1, 2))).tolist(),
        'global_rot': sam3d_output.get('global_rot', np.zeros(3)).tolist(),
        'body_pose': sam3d_output.get('body_pose_params', np.zeros(1)).tolist(),
    }
    
    return result


def process_video(video_path, estimator, output_dir, skip_frames=1):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∏–¥–µ–æ –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç 3D –ø–æ–∑—É –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–∞–¥—Ä–∞
    
    Args:
        video_path: –ø—É—Ç—å –∫ –≤–∏–¥–µ–æ
        estimator: SAM3DBodyEstimator
        output_dir: –ø–∞–ø–∫–∞ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        skip_frames: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∫–∞–∂–¥—ã–π N-–π –∫–∞–¥—Ä (–¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è)
    """
    
    os.makedirs(output_dir, exist_ok=True)
    
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    print(f"üìπ Video: {width}x{height}, {fps:.1f} fps, {total_frames} frames")
    print(f"‚è≠Ô∏è Processing every {skip_frames} frame(s)")
    
    all_poses = []
    frame_indices = []
    vertices_list = []
    
    frame_count = 0
    processed_count = 0
    
    # Bbox –¥–ª—è –≤—Å–µ–≥–æ –∫–∞–¥—Ä–∞ (—Ç—Ä–µ–Ω–µ—Ä –∑–∞–Ω–∏–º–∞–µ—Ç –≤–µ—Å—å –∫–∞–¥—Ä)
    full_frame_bbox = np.array([[0, 0, width, height]], dtype=np.float32)
    
    with tqdm(total=total_frames // skip_frames, desc="Extracting 3D Pose") as pbar:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            if frame_count % skip_frames == 0:
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º BGR -> RGB
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                
                try:
                    # –ó–∞–ø—É—Å–∫–∞–µ–º inference
                    outputs = estimator.process_one_image(
                        frame_rgb,
                        bboxes=full_frame_bbox,
                        inference_type="body"  # –¢–æ–ª—å–∫–æ —Ç–µ–ª–æ, –±–µ–∑ —Ä—É–∫ (–±—ã—Å—Ç—Ä–µ–µ)
                    )
                    
                    if outputs and len(outputs) > 0:
                        # –ë–µ—Ä—ë–º –ø–µ—Ä–≤–æ–≥–æ (–µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ) —á–µ–ª–æ–≤–µ–∫–∞
                        pose_data = sam3d_to_mediapipe_format(outputs[0])
                        
                        if pose_data:
                            all_poses.append(pose_data)
                            frame_indices.append(frame_count)
                            
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º vertices –¥–ª—è 3D –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
                            if 'pred_vertices' in outputs[0]:
                                vertices_list.append(outputs[0]['pred_vertices'])
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è Frame {frame_count}: {e}")
                
                processed_count += 1
                pbar.update(1)
            
            frame_count += 1
    
    cap.release()
    
    print(f"\n‚úÖ Processed {processed_count} frames")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    
    # 1. JSON —Å –ø–æ–∑–∞–º–∏ –¥–ª—è –±—Ä–∞—É–∑–µ—Ä–∞ (—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å MediaPipe)
    poses_json = {
        'fps': fps / skip_frames,  # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π FPS
        'original_fps': fps,
        'skip_frames': skip_frames,
        'width': width,
        'height': height,
        'total_frames': len(all_poses),
        'frame_indices': frame_indices,
        'poses': all_poses
    }
    
    json_path = os.path.join(output_dir, 'trainer_poses.json')
    with open(json_path, 'w') as f:
        json.dump(poses_json, f)
    print(f"üíæ Saved: {json_path}")
    
    # 2. NPZ —Å –ø–æ–ª–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è 3D –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    npz_path = os.path.join(output_dir, 'trainer_poses.npz')
    
    save_dict = {
        'fps': fps / skip_frames,
        'original_fps': fps,
        'skip_frames': skip_frames,
        'width': width,
        'height': height,
        'frame_indices': np.array(frame_indices),
        'faces': estimator.faces,  # –ú–µ—à-—Ç–æ–ø–æ–ª–æ–≥–∏—è –¥–ª—è —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞
    }
    
    # –î–æ–±–∞–≤–ª—è–µ–º poses –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –º–∞—Å—Å–∏–≤—ã
    if all_poses:
        # –ò–∑–≤–ª–µ–∫–∞–µ–º joints_3d –≤ numpy –º–∞—Å—Å–∏–≤
        joints_3d_list = [p.get('joints_3d', []) for p in all_poses]
        if joints_3d_list and joints_3d_list[0]:
            save_dict['joints_3d'] = np.array(joints_3d_list)
        
        keypoints_3d_list = [p.get('keypoints_3d', []) for p in all_poses]
        if keypoints_3d_list and keypoints_3d_list[0]:
            save_dict['keypoints_3d'] = np.array(keypoints_3d_list)
    
    if vertices_list:
        save_dict['vertices'] = np.array(vertices_list)
    
    np.savez_compressed(npz_path, **save_dict)
    print(f"üíæ Saved: {npz_path}")
    
    return json_path, npz_path


def main():
    parser = argparse.ArgumentParser(description='Extract 3D Pose from Video using SAM 3D Body')
    parser.add_argument('--video', type=str, required=True, help='Path to input video')
    parser.add_argument('--output', type=str, default='output/poses', help='Output directory')
    parser.add_argument('--checkpoint', type=str, default='checkpoints/sam-3d-body-dinov3/model.ckpt',
                        help='Path to SAM 3D Body checkpoint')
    parser.add_argument('--mhr', type=str, default='checkpoints/sam-3d-body-dinov3/assets/mhr_model.pt',
                        help='Path to MHR model')
    parser.add_argument('--skip', type=int, default=1, help='Process every N frames')
    parser.add_argument('--device', type=str, default='cuda', help='Device (cuda/cpu)')
    
    args = parser.parse_args()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–π–ª—ã
    if not os.path.exists(args.video):
        print(f"‚ùå Video not found: {args.video}")
        return
    
    if not os.path.exists(args.checkpoint):
        print(f"‚ùå Checkpoint not found: {args.checkpoint}")
        return
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    device = args.device
    if device == 'cuda' and not torch.cuda.is_available():
        print("‚ö†Ô∏è CUDA not available, using CPU")
        device = 'cpu'
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
    estimator = setup_estimator(args.checkpoint, args.mhr, device)
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–∏–¥–µ–æ
    json_path, npz_path = process_video(
        args.video, 
        estimator, 
        args.output,
        skip_frames=args.skip
    )
    
    print(f"\nüéâ Done!")
    print(f"üìÅ JSON (for browser): {json_path}")
    print(f"üìÅ NPZ (for 3D viz): {npz_path}")


if __name__ == '__main__':
    main()
