# üöÄ Lightning.AI - –ü–æ–ª–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞ Smart Mirror

## –ü—Ä–æ–µ–∫—Ç: pers.coach ‚Äî Shadow Training Platform

**–¶–µ–ª—å:** –°–æ–∑–¥–∞—Ç—å –ø–ª–∞—Ç—Ñ–æ—Ä–º—É –≥–¥–µ –∫–ª–∏–µ–Ω—Ç –ø–æ–≤—Ç–æ—Ä—è–µ—Ç –¥–≤–∏–∂–µ–Ω–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ —á–µ—Ä–µ–∑ –≤–µ–±–∫–∞–º–µ—Ä—É, —Å –≤–∏–∑—É–∞–ª—å–Ω—ã–º —Ñ–∏–¥–±–µ–∫–æ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ–∑.

---

## üì¶ –ß—Ç–æ –Ω—É–∂–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞ GPU

### –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:
- `squat_cropped.mp4` ‚Äî –≤–∏–¥–µ–æ —Ç—Ä–µ–Ω–µ—Ä–∞ (602x722, 30fps, 5793 –∫–∞–¥—Ä–∞, ~3 –º–∏–Ω)

### –í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:

| –§–∞–π–ª | –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ | –§–æ—Ä–º–∞—Ç |
|------|------------|--------|
| `trainer_transparent.webm` | –í–∏–¥–µ–æ –¥–ª—è –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã | VP9 + alpha (62 MB) |
| `trainer_poses.json` | 3D –ø–æ–∑—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è | JSON —Å landmarks |
| `trainer_poses.npz` | –î–∞–Ω–Ω—ã–µ –¥–ª—è 3D —á–µ–ª–æ–≤–µ—á–∫–∞ | NPZ (vertices, faces) |

---

## üéØ –ó–∞–¥–∞—á–∞ 1: –£–¥–∞–ª–µ–Ω–∏–µ —Ñ–æ–Ω–∞ + Color Correction (‚úÖ –ì–û–¢–û–í–û)

**–°—Ç–∞—Ç—É—Å:** –í—ã–ø–æ–ª–Ω–µ–Ω–æ –ª–æ–∫–∞–ª—å–Ω–æ –Ω–∞ CPU (44 –º–∏–Ω), —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ `output/trainer_transparent.webm`

**Pipeline:**
1. MODNet ‚Äî —É–¥–∞–ª–µ–Ω–∏–µ —Ñ–æ–Ω–∞
2. V11 Color Correction ‚Äî —Å—Ç—É–¥–∏–π–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
3. FFmpeg ‚Äî WebM —Å –∞–ª—å—Ñ–∞-–∫–∞–Ω–∞–ª–æ–º

**V11 –Ω–∞—Å—Ç—Ä–æ–π–∫–∏:**
```python
brightness_boost = 0.08
contrast = 1.12
lab_lift = 52
warmth = {red_boost: 1.03, blue_reduce: 0.97}
```

---

## ü¶¥ –ó–∞–¥–∞—á–∞ 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ 3D –ø–æ–∑—ã (SAM 3D Body)

**–°—Ç–∞—Ç—É—Å:** ‚è≥ –ù—É–∂–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –Ω–∞ GPU

**–¶–µ–ª—å:** –ò–∑–≤–ª–µ—á—å 3D –ø–æ–∑—É —Ç—Ä–µ–Ω–µ—Ä–∞ –∏–∑ –∫–∞–∂–¥–æ–≥–æ –∫–∞–¥—Ä–∞ –¥–ª—è:
1. –°—Ä–∞–≤–Ω–µ–Ω–∏—è —Å –ø–æ–∑–æ–π —é–∑–µ—Ä–∞ (MediaPipe –≤ –±—Ä–∞—É–∑–µ—Ä–µ)
2. –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è 3D —á–µ–ª–æ–≤–µ—á–∫–∞ —Ç—Ä–µ–Ω–µ—Ä–∞
3. –ù–∞–ª–æ–∂–µ–Ω–∏—è —Å–∫–µ–ª–µ—Ç–æ–≤ —Ç—Ä–µ–Ω–µ—Ä–∞ –∏ —é–∑–µ—Ä–∞

---

## üîß –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ Lightning.AI

### –®–∞–≥ 1: –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏

```bash
# SAM 3D Body –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
pip install pytorch-lightning pyrender opencv-python yacs scikit-image einops timm dill pandas rich hydra-core pyrootutils networkx==3.2.1 roma joblib huggingface_hub

# Detectron2 (–¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ —á–µ–ª–æ–≤–µ–∫–∞)
pip install 'git+https://github.com/facebookresearch/detectron2.git@a1ce2f9' --no-build-isolation --no-deps

# –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
python -c "import torch; print(f'PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}, GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\"}')"
```

### –®–∞–≥ 2: –ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ SAM 3D Body

```bash
git clone https://github.com/facebookresearch/sam-3d-body.git
```

### –®–∞–≥ 3: –ß–µ–∫–ø–æ–∏–Ω—Ç—ã (‚ö†Ô∏è –Ω—É–∂–µ–Ω –¥–æ—Å—Ç—É–ø HuggingFace)

```bash
# –õ–æ–≥–∏–Ω –≤ HuggingFace
huggingface-cli login

# –°–∫–∞—á–∏–≤–∞–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç—ã (~800 MB)
huggingface-cli download facebook/sam-3d-body-dinov3 --local-dir checkpoints/sam-3d-body-dinov3
```

**–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞:** –ó–∞–≥—Ä—É–∑–∏—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç—ã —Å –ª–æ–∫–∞–ª—å–Ω–æ–π –º–∞—à–∏–Ω—ã (—É–∂–µ —Å–∫–∞—á–∞–Ω—ã –≤ `/Users/user/Documents/pers2/checkpoints/`)

### –®–∞–≥ 4: –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤

–ó–∞–≥—Ä—É–∑–∏ —Å –ª–æ–∫–∞–ª—å–Ω–æ–π –º–∞—à–∏–Ω—ã:
- `squat_cropped.mp4` ‚Äî –≤–∏–¥–µ–æ —Ç—Ä–µ–Ω–µ—Ä–∞
- `extract_3d_pose.py` ‚Äî —Å–∫—Ä–∏–ø—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏

### –®–∞–≥ 5: –ó–∞–ø—É—Å–∫

```bash
python extract_3d_pose.py \
    --video squat_cropped.mp4 \
    --output output/poses \
    --checkpoint checkpoints/sam-3d-body-dinov3/model.ckpt \
    --mhr checkpoints/sam-3d-body-dinov3/assets/mhr_model.pt \
    --skip 2 \
    --device cuda
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `--skip 2` ‚Äî –∫–∞–∂–¥—ã–π 2-–π –∫–∞–¥—Ä (–±—ã—Å—Ç—Ä–µ–µ, 15 fps —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö)
- `--skip 1` ‚Äî –≤—Å–µ –∫–∞–¥—Ä—ã (–ø–æ–ª–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –º–µ–¥–ª–µ–Ω–Ω–µ–µ)

**–í—Ä–µ–º—è:** ~15-30 –º–∏–Ω –Ω–∞ T4 GPU

---

## üìä –§–æ—Ä–º–∞—Ç –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

### trainer_poses.json (–¥–ª—è –±—Ä–∞—É–∑–µ—Ä–∞)
```json
{
  "fps": 15,
  "width": 602,
  "height": 722,
  "total_frames": 2896,
  "poses": [
    {
      "joints_3d": [[x, y, z], ...],
      "keypoints_2d": [[x, y], ...],
      "global_rot": [rx, ry, rz]
    },
    ...
  ]
}
```

### trainer_poses.npz (–¥–ª—è 3D –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏)
```python
{
  "fps": 15,
  "joints_3d": np.array([frames, joints, 3]),
  "vertices": np.array([frames, vertices, 3]),
  "faces": np.array([faces, 3])  # Mesh topology
}
```

---

## üñ•Ô∏è –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ –±—Ä–∞—É–∑–µ—Ä–µ

### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ–∑
```javascript
// –ó–∞–≥—Ä—É–∂–∞–µ–º JSON —Å –ø–æ–∑–∞–º–∏ —Ç—Ä–µ–Ω–µ—Ä–∞
const trainerPoses = await fetch('trainer_poses.json').then(r => r.json());

// –ü–æ–ª—É—á–∞–µ–º –ø–æ–∑—É —é–∑–µ—Ä–∞ —á–µ—Ä–µ–∑ MediaPipe
const userPose = await poseLandmarker.detect(webcamFrame);

// –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º
const matchScore = comparePoses(
    trainerPoses.poses[currentFrame].joints_3d,
    userPose.worldLandmarks
);
```

### 3D –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è (Three.js)
```javascript
// –ó–∞–≥—Ä—É–∂–∞–µ–º NPZ (—á–µ—Ä–µ–∑ jszip + np-loader)
const data = await loadNPZ('trainer_poses.npz');

// –°–æ–∑–¥–∞—ë–º –º–µ—à
const geometry = new THREE.BufferGeometry();
geometry.setAttribute('position', new THREE.Float32BufferAttribute(data.vertices[frame], 3));
geometry.setIndex(data.faces.flat());

const mesh = new THREE.Mesh(geometry, material);
scene.add(mesh);
```

---

## üîÑ –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞: MediaPipe Pose (–µ—Å–ª–∏ –Ω–µ—Ç SAM 3D Body)

```python
import mediapipe as mp
import cv2
import json

mp_pose = mp.solutions.pose
pose = mp_pose.Pose(
    static_image_mode=False,
    model_complexity=2,  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
    enable_segmentation=False,
    min_detection_confidence=0.5
)

cap = cv2.VideoCapture('squat_cropped.mp4')
poses = []

while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    results = pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
    
    if results.pose_world_landmarks:
        landmarks = [
            {'x': lm.x, 'y': lm.y, 'z': lm.z}
            for lm in results.pose_world_landmarks.landmark
        ]
        poses.append(landmarks)

with open('trainer_poses_mediapipe.json', 'w') as f:
    json.dump({'poses': poses}, f)
```

**–ü–ª—é—Å—ã MediaPipe:**
- –ù–µ —Ç—Ä–µ–±—É–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ HuggingFace
- –ë—ã—Å—Ç—Ä–µ–µ (~3-5 –º–∏–Ω –Ω–∞ GPU)
- –°–æ–≤–º–µ—Å—Ç–∏–º–æ —Å –±—Ä–∞—É–∑–µ—Ä–Ω—ã–º MediaPipe

**–ú–∏–Ω—É—Å—ã:**
- –ù–µ—Ç –ø–æ–ª–Ω–æ–≥–æ –º–µ—à–∞ (—Ç–æ–ª—å–∫–æ —Å–∫–µ–ª–µ—Ç)
- –ú–µ–Ω–µ–µ —Ç–æ—á–Ω–æ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –ø–æ–∑

---

## üìÅ –ò—Ç–æ–≥–æ–≤—ã–µ —Ñ–∞–π–ª—ã –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è

–ü–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –æ–±–µ–∏—Ö –∑–∞–¥–∞—á —Å–∫–∞—á–∞–π:

```
output/
‚îú‚îÄ‚îÄ trainer_transparent.webm    # 62 MB - –≤–∏–¥–µ–æ —Å –∞–ª—å—Ñ–∞
‚îú‚îÄ‚îÄ trainer_frames/             # ~2 GB - PNG –∫–∞–¥—Ä—ã (backup)
‚îî‚îÄ‚îÄ poses/
    ‚îú‚îÄ‚îÄ trainer_poses.json      # ~10 MB - –¥–ª—è –±—Ä–∞—É–∑–µ—Ä–∞
    ‚îî‚îÄ‚îÄ trainer_poses.npz       # ~50 MB - –¥–ª—è 3D
```

